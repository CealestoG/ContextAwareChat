# Context-Aware Chat Orchestration System

A multi-agent orchestration system for intelligent and context-aware conversations.  
It uses modular agents, coordinated through Microsoft's AutoGen framework, and supports offline operation using local LLMs via Ollama.  
Retrieval-Augmented Generation (RAG) enhances factual accuracy in responses.

---

##  Architecture Overview


  overview: |
    User Query
       â”‚
       â–¼
    QueryClassifier Agent â”€â”€â–º Retriever Agent â”€â”€â–º Domain Expert Agent
            â”‚                          â”‚                    â”‚
            â–¼                          â–¼                    â–¼
    Refinement Agent â—„â”€â”€â”€â”€â”€â”€â”€â”€ Response Aggregator â—„â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    Final Response
--


---

## ğŸ§© Agents

| Agent              | Role                                                                 |
|--------------------|----------------------------------------------------------------------|
| QueryClassifier     | Classifies incoming queries into categories for intelligent routing |
| RetrieverAgent      | Fetches relevant documents via RAG pipeline from local vector DB    |
| DomainExpertAgent   | Uses LLM to provide domain-specific answers                         |
| ResponseAggregator  | Merges responses from different agents into a cohesive reply        |
| RefinementAgent     | Final pass for fluency, correctness, and clarity                    |

---

## ğŸ› ï¸ Tech Stack

- **Languages:** Python 3.10+
- **Frameworks:** AutoGen, LangChain
- **Tools:** Ollama, Chroma or FAISS
- **Formats:** Markdown, YAML, JSON

---

## ğŸ“ Directory Structure

  context_aware_chat/
  â”œâ”€â”€ agents/
  â”‚   â”œâ”€â”€ query_classifier.py
  â”‚   â”œâ”€â”€ retriever_agent.py
  â”‚   â”œâ”€â”€ domain_expert_agent.py
  â”‚   â”œâ”€â”€ response_aggregator.py
  â”‚   â””â”€â”€ refinement_agent.py
  â”œâ”€â”€ rag/
  â”‚   â”œâ”€â”€ document_loader.py
  â”‚   â”œâ”€â”€ vector_store.py
  â”‚   â”œâ”€â”€ retriever.py
  â”‚   â””â”€â”€ rag_pipeline.py
  â”œâ”€â”€ orchestration.py
  â”œâ”€â”€ llm_interface.py
  â”œâ”€â”€ README.md
  â””â”€â”€ requirements.txt
--

##  Offline Capability

All LLM-based processing happens locally using Ollama with models like Mistral or LLaMA3.

**Benefits:**
- Privacy-preserving  
- API-key free  
- Air-gapped compatible  
- Cost-effective  

---

##  Features

- Modular multi-agent architecture  
- Retrieval-Augmented Generation (RAG)  
- Local LLM integration  
- Clear separation of agent logic  
- Extensible and testable pipeline  

---

##  How to Run

1. Install dependencies:  
2. ollama run mistral
3.python orchestration.py
---
## ğŸ§­ Future Improvements

- GUI integration using Gradio or Streamlit  
- Add persistent memory with SQLite or MongoDB  
- Multi-turn conversational history  
- Plug-and-play domain expert modules  

---

## ğŸ‘¨â€ğŸ’» Author

**CealestoG**  
Built for research, learning, and real-world deployment.  
GitHub: [https://github.com/CealestoG](https://github.com/CealestoG)

